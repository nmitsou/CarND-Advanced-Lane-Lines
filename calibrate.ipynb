{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Advanced Lane Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udacity - Self-Driving Car NanoDegree\n",
    "\n",
    "In this project, an advanced lane detection algorithm was impelemented. Below an overview of the pipeline and relevant results are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Camera calibration and distortion matrix calculation\n",
    "2. For a given image do\n",
    "3. Unistort image\n",
    "4. Transform image street perpective\n",
    "5. Apply sobel, color and s-channel filtering\n",
    "6. Slice the image and create histograms of lane points per slice\n",
    "7. Smooth the histograms and extract local maximum points\n",
    "8. If two maximum's are found, one on the left and one on the right, fit a polygonal on each side\n",
    "9. Calculate curvature per polynomial\n",
    "10. Filter polynomials if curvatures do not match\n",
    "11. Invert project lanes to original image space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f0b8ccda812b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import glob\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from scipy.signal import argrelextrema\n",
    "from moviepy.editor import VideoFileClip\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apply distortion-correction to raw image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load calibration images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load all calibration images to the array: image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nx = 9\n",
    "ny = 6\n",
    "\n",
    "image_list = []\n",
    "for filename in glob.glob('camera_cal/*.jpg'):\n",
    "    image_list.append(cv2.imread(filename))\n",
    "    \n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate to generate coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I convert the images to grayscale and then apply the findChessboardCorners to get all corners. The calibrateCamera method of opencv eventually generates the camera matrix and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "objpoints = []\n",
    "imgpoints = []\n",
    "    \n",
    "objp = np.zeros((nx * ny, 3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
    "    \n",
    "for image in image_list:\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    if ret:\n",
    "        imgpoints.append(corners)\n",
    "        objpoints.append(objp)\n",
    "\n",
    "rms, camera_matrix, dist_coefs, rvecs, tvecs = cv2.calibrateCamera( objpoints, \n",
    "                                                        imgpoints, \n",
    "                                                        gray.shape[::-1], \n",
    "                                                        None, \n",
    "                                                        None)\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The undistort method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cv2.undistort method can undistort an image give the camera matrix and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def undistort(img):\n",
    "    img_undistorted = cv2.undistort(img, camera_matrix, dist_coefs, None, camera_matrix)\n",
    "    return img_undistorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and display a test image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The undistort seems to work fine since the image on the right has been corrected to contain straight lines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "image = cv2.cvtColor(image_list[7], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_image = np.concatenate((image, undistort(image)),axis=1) \n",
    "plt.imshow(plot_image)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of undistortion on an image collected while driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "image = cv2.imread('./hard2/outfile993.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plot_image = np.concatenate((image, undistort(image)),axis=1) \n",
    "plt.imshow(plot_image)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. Provide an example of a binary image result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a combination of color, sobel x and s-channel filtering (from HLS) to generate a binary image (as shown below)\n",
    "The formula used is: +TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define methods for sobel_x filtering, HLS thresholding on s-channel and RGB filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_sobel_x(gray, thresh_min=20, thresh_max=200):\n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0) \n",
    "    abs_sobelx = np.absolute(sobelx) \n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "\n",
    "    # Apply a threshold\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "\n",
    "    return sxbinary\n",
    "\n",
    "def apply_hls_threshold(image):\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    #plt.imshow(s_channel)\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[((s_channel >= 130) & (s_channel <= 240)) | ((l_channel >= 220) & (l_channel <= 255))] = 1\n",
    "    #print(s_binary)\n",
    "\n",
    "    return s_binary\n",
    "\n",
    "def get_color_selection(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    lower_yellow = np.array([0, 90, 90], dtype=np.uint8)\n",
    "    upper_yellow = np.array([210,250,255], dtype=np.uint8)\n",
    "    \n",
    "    upper_white =np.array([190,190,190], dtype=np.uint8)\n",
    "    lower_white = np.array([255,255,255], dtype=np.uint8)\n",
    "    \n",
    "    # Get the white pixels from the original image\n",
    "    mask_white = cv2.inRange(image, upper_white, lower_white)\n",
    "\n",
    "    # Get the yellow pixels from the HSV image\n",
    "    mask_yellow = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "    \n",
    "    # Bitwise-OR white and yellow mask\n",
    "    mask = cv2.bitwise_or(mask_white, mask_yellow)\n",
    "    return mask\n",
    "\n",
    "def threshold_image(image):\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    #plt.imshow(gray)\n",
    "\n",
    "    binary1 = apply_sobel_x(gray)\n",
    "    binary3 = apply_hls_threshold(image)\n",
    "    binary4 = get_color_selection(image)\n",
    "    \n",
    "    plt.imshow(binary1)\n",
    "    #f, (f1, f2) = plt.subplots(1, 2)\n",
    "    #f1.imshow(binary1)\n",
    "    #f2.imshow(binary4)\n",
    "    \n",
    "    # Combine all thresholds and filters\n",
    "    combined_binary = np.zeros_like(binary1)\n",
    "    combined_binary[((binary1 == 1) & (binary4 == 1)) | (binary3 == 1) ] = 1\n",
    "    \n",
    "    return combined_binary\n",
    "\n",
    "def merge_images(img1, img2):\n",
    "    return cv2.addWeighted(img1, 0.4, img2, 0.6, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example image filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lanes_image = threshold_image(project(image))\n",
    "#print(lanes_image.shape)\n",
    "# Two subplots, the axes array is 1-d\n",
    "f, (f1, f2) = plt.subplots(1, 2)\n",
    "f1.imshow(project(image))\n",
    "f1.set_title('Projected image')\n",
    "f2.imshow(lanes_image)\n",
    "f2.set_title('Filtered image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformation points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find src points for projection\n",
    "image = undistort(image)\n",
    "h = image.shape[0]\n",
    "\n",
    "src = np.int32([[230,695],[578,455],[704,455],[1065,695]])\n",
    "dest = np.int32([[250,h],[250,0],[1050,0],[1050,h]])\n",
    "#pts = pts.reshape((-1,1,2))\n",
    "lanes_marked = np.copy(image)\n",
    "lanes_marked = cv2.polylines(lanes_marked,[src],True,(255,0,0), thickness=3)\n",
    "lanes_marked = cv2.polylines(lanes_marked,[dest],True,(255,0,0), thickness=3)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(lanes_marked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define project and invert_project methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project(image):\n",
    "    \n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    src = np.float32([[230,695],[578,455],[704,455],[1065,695]])\n",
    "    dst = np.float32([[250,h],[250,0],[1050,0],[1050,h]]) \n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "    warped = cv2.warpPerspective(image, M, (w,h))\n",
    "    return warped\n",
    "\n",
    "def invert_project(image):\n",
    "    \n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    src = np.float32([[230,695],[578,455],[704,455],[1065,695]])\n",
    "    dst = np.float32([[250,h],[250,0],[1050,0],[1050,h]]) \n",
    "\n",
    "    M = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "    warped = cv2.warpPerspective(image, M, (w,h))\n",
    "    empty = np.zeros(warped.shape, dtype=np.uint8)\n",
    "    final = np.dstack((warped, empty, empty))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example images for perspective transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_image = np.concatenate((image, project(image)),axis=1) \n",
    "plt.imshow(plot_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define method to extract lane points from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_lane_points(image):\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    window_size = 40\n",
    "    left_lanes_x = []\n",
    "    left_lanes_y = []\n",
    "    right_lanes_x = []\n",
    "    right_lanes_y = []\n",
    "    for window_index in range(1,int(height / window_size)):\n",
    "        window_lanes = image[window_index*window_size:(window_index+1)*window_size, :]\n",
    "                           \n",
    "        histogram = np.sum(window_lanes, axis=0)\n",
    "\n",
    "        window = signal.general_gaussian(49, p=3, sig=40)\n",
    "        filtered = signal.fftconvolve(window, histogram)\n",
    "        filtered = (np.average(histogram) / np.average(filtered)) * filtered\n",
    "        histogram = np.roll(filtered, -25)\n",
    "\n",
    "        #plt.plot(histogram)\n",
    "        indices = argrelextrema(histogram, np.greater)\n",
    "\n",
    "        data_y_left = []\n",
    "        data_x_left = []\n",
    "        data_y_right = []\n",
    "        data_x_right = []\n",
    "    \n",
    "        for i in indices[0]:\n",
    "            if histogram[i] > 13:\n",
    "                if i > 600:\n",
    "                    data_x_right.append(i)\n",
    "                    data_y_right.append(window_size*window_index)\n",
    "                else:\n",
    "                    data_x_left.append(i)\n",
    "                    data_y_left.append(window_size*window_index)\n",
    "            \n",
    "        if len(data_x_left) == 1:\n",
    "            left_lanes_x.append(data_x_left[0])\n",
    "            left_lanes_y.append(data_y_left[0])\n",
    "\n",
    "        if len(data_x_right) == 1:\n",
    "            right_lanes_x.append(data_x_right[0])\n",
    "            right_lanes_y.append(data_y_right[0])\n",
    "\n",
    "    return left_lanes_x, left_lanes_y, right_lanes_x, right_lanes_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of detected lane points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_lanes_x, left_lanes_y, right_lanes_x, right_lanes_y = find_lane_points(lanes_image)\n",
    "plt.plot(right_lanes_x, right_lanes_y, 'og')\n",
    "plt.plot(left_lanes_x, left_lanes_y, 'oy')\n",
    "\n",
    "plt.imshow(project(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvature and car distance to lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "def calculate_curvature(fit) :\n",
    "    return ((ym_per_pix**2 + xm_per_pix**2*(2*fit[0]*720 + fit[1])**2)**1.5)/(2*xm_per_pix*ym_per_pix*fit[0])\n",
    "    \n",
    "def calc_base_dist(self, fit) :\n",
    "    y = img_dim[1]\n",
    "    dist = -img_dim[0]/2\n",
    "    for i in range(self.poly_order+1) :\n",
    "        dist += fit[i]*y**(self.poly_order-i)\n",
    "    return dist*xm_per_pix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find lines method to fit polylines, export their detected points and return a layered image with left and right curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_lines(image, left_lanes_x, left_lanes_y, right_lanes_x, right_lanes_y):\n",
    "    #print(left_lanes_x)\n",
    "    if len(left_lanes_x) < 3:\n",
    "        return False, 0, 0, 0\n",
    "    #print(right_lanes_x)\n",
    "    if len(right_lanes_x) < 3:\n",
    "        return False, 0, 0, 0\n",
    "    \n",
    "    overlay_lanes = np.copy(image)\n",
    "\n",
    "    line = np.polyfit(left_lanes_y, left_lanes_x, deg=2)\n",
    "    p = np.poly1d(line)\n",
    "    x = list(range(0, image.shape[0]))\n",
    "    y = list(map(int, p(x)))\n",
    "    line1_pts = np.array([[_y,_x] for _x, _y in zip(x, y)])\n",
    "    line1_pts = line1_pts.reshape((-1,1,2))\n",
    "    cv2.polylines(overlay_lanes, np.int32([line1_pts]), False, color=(255,0,0), thickness=50)\n",
    "    left_r = calculate_curvature(line)\n",
    "\n",
    "    line = np.polyfit(right_lanes_y, right_lanes_x, deg=2)\n",
    "    p = np.poly1d(line)\n",
    "    x = list(range(0, image.shape[0]))\n",
    "    y = list(map(int, p(x)))\n",
    "    line2_pts = np.array([[_y,_x] for _x, _y in zip(x, y)])\n",
    "    line2_pts = line2_pts.reshape((-1,1,2))\n",
    "    cv2.polylines(overlay_lanes, np.int32([line2_pts]), False, color=(255,0,0), thickness=50)\n",
    "    right_r = calculate_curvature(line)\n",
    "\n",
    "    top_points = [line1_pts[-1], line2_pts[-1]]\n",
    "    base_points = [line1_pts[0], line2_pts[0]]\n",
    "    \n",
    "    # Fill in the detected lane\n",
    "    cv2.fillPoly(overlay_lanes, [np.concatenate((line2_pts, line1_pts, top_points, base_points))], color=(100,200,150))\n",
    "    #cv2.putText(overlay_lanes, 'Vehicle is '+position+'m left of center', (50, 100), font, 1, (255, 255, 255), 2)\n",
    "#    cv2.putText(overlay_lanes, 'Radius left: '+str(left_r)+' Radius right: '+str(right_r), (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 100), 2)\n",
    "#    cv2.putText(final_merged, str(i), (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "#    print(left_r)\n",
    "    plt.imshow(overlay_lanes)\n",
    "    return True, overlay_lanes, left_r, right_r\n",
    "\n",
    "success, overlay_lanes, left_r, right_r = find_lines(lanes_image, left_lanes_x, left_lanes_y, right_lanes_x, right_lanes_y)\n",
    "\n",
    "if success == False:\n",
    "    print('success = '+str(success))\n",
    "    print('Failed to extract lanes')\n",
    "else:\n",
    "    f, (f1, f2) = plt.subplots(1, 2)\n",
    "    f1.imshow(project(image))\n",
    "    f1.set_title('Projected image')\n",
    "    f2.imshow(overlay_lanes)\n",
    "    f2.set_title('Lane area')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of an output image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = invert_project(overlay_lanes)\n",
    "#print(overlay_lanes.shape)\n",
    "#print(image.shape)\n",
    "final_merged = merge_images(final, image)\n",
    "#print(final)\n",
    "plt.imshow(final_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use a history of the last history_size frames to make more stable detections of the lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def history(history, current):\n",
    "    size = len(history)\n",
    "    if size < history_size:\n",
    "        return current\n",
    "    if history_size == 1:\n",
    "        return current\n",
    "    output = current\n",
    "    for i in range(1, history_size + 1):\n",
    "        output = output + history[size - i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main pipeline as explained on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_left_x, history_left_y, history_right_x, history_right_y = [],[],[],[]\n",
    "old_lanes_image = []\n",
    "i = 0\n",
    "def process(image, useHistory = True):\n",
    "    global old_lanes_image\n",
    "    \n",
    "    if i > 970 and i < 1010:\n",
    "#    if i == 563 or i == 614 or i == 610 or i == 617 or i == 1025 or i == 1032 or i == 1047:\n",
    "        scipy.misc.toimage(image, cmin=0.0, cmax=255).save('hard2/outfile'+str(i)+'.jpg')\n",
    "    \n",
    "    undistorted = undistort(image)\n",
    "    projected = project(undistorted)\n",
    "    \n",
    "    lanes_image = threshold_image(projected)\n",
    "    \n",
    "    left_lanes_x, left_lanes_y, right_lanes_x, right_lanes_y = find_lane_points(lanes_image)\n",
    "    \n",
    "    history_left_x.append(left_lanes_x)\n",
    "    history_left_y.append(left_lanes_y)\n",
    "    history_right_x.append(right_lanes_x)\n",
    "    history_right_y.append(right_lanes_y)\n",
    "    \n",
    "    history_length = len(history_left_x)\n",
    "    if useHistory and history_length < history_size:\n",
    "        #cv2.putText(image,\"Collecting initial data\", (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        return image\n",
    "\n",
    "    left_x = history(history_left_x, left_lanes_x)\n",
    "    left_y = history(history_left_y, left_lanes_y)\n",
    "    right_x = history(history_right_x, right_lanes_x)\n",
    "    right_y = history(history_right_y, right_lanes_y)\n",
    "    success, overlay_lanes, left_r, right_r = find_lines(lanes_image, left_x, left_y, right_x, right_y)\n",
    "\n",
    "    if len(old_lanes_image) == 0:\n",
    "        old_lanes_image = overlay_lanes\n",
    "        \n",
    "    if success == False:\n",
    "        overlay_lanes = old_lanes_image\n",
    "    if abs(left_r) < 280 or abs(right_r) < 280:\n",
    "        overlay_lanes = old_lanes_image\n",
    "    if left_r * right_r < 0:\n",
    "        if abs(abs(left_r) - abs(right_r)) >2000 and (abs(left_r) < 1000 or abs(right_r) < 1000):\n",
    "            overlay_lanes = old_lanes_image\n",
    "        \n",
    "    final = invert_project(overlay_lanes)\n",
    "    final_merged = merge_images(final, image)\n",
    "    \n",
    "    old_lanes_image = overlay_lanes\n",
    "    \n",
    "    global i\n",
    "    cv2.putText(final_merged, str(i), (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(final_merged, 'Radius left: '+str(abs(left_r))+' Radius right: '+str(abs(right_r)), (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 100), 2)\n",
    "    \n",
    "\n",
    "    i = i + 1\n",
    "    return final_merged\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input video and generate output video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "white_output = './project_video_labeled.mp4'\n",
    "clip1 = VideoFileClip(\"./project_video.mp4\")\n",
    "white_clip = clip1.fl_image(process) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further results for some edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global i\n",
    "i = 0\n",
    "for filename in glob.glob('hard/*.jpg'):\n",
    "    plt.figure(figsize=(6,6))\n",
    "\n",
    "    print(filename)\n",
    "    out = cv2.cvtColor(process(cv2.imread(filename), False), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(out)\n",
    "    i = i + 1\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
